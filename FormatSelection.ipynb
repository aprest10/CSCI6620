{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Driven Format Selection\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure CUDA is enabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"CUDA device name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from helpers import (\n",
    "    SparseMatrixType as smt,\n",
    "    SparseMatrixGenerator as smg,\n",
    "    SparseMatrixVisualizer as smv,\n",
    "    SparseMatrixBenchmark as smb,\n",
    "    SparseMatrixAnalysis as sma,\n",
    "    SparseMatrixPredictor as smp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "generator = smg()\n",
    "\n",
    "train_files, val_files = generator.generate_train_val_sets(\n",
    "    sizes=[512, 1024, 2048],\n",
    "    types=[smt.RANDOM, smt.BANDED, smt.BLOCK_DIAGONAL, smt.TRIDIAGONAL, smt.CHECKERBOARD],\n",
    "    num_train=500,\n",
    "    num_val=100,\n",
    "    train_prefix=\"train_matrix\",\n",
    "    val_prefix=\"test_matrix\",\n",
    "    filename_fmt=\"{prefix}_size{size}_{type}_{index:08d}.mtx\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(train_files)} training files\")\n",
    "print(f\"Generated {len(val_files)} validation files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "benchmark_training = smb(\"./train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = benchmark_training.results_df\n",
    "df_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_training.plot_results(\"results_plot.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_training = sma(df_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_formats_training = analyzer_training.get_optimal_formats()\n",
    "optimal_formats_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_training.print_analysis_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Develope tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = smp(optimal_formats_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = predictor.model.score(predictor.X_test, predictor.y_test)\n",
    "print(f\"Model accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.visualize_tree(\"decision_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.get_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "metrics = predictor.evaluate_model()\n",
    "\n",
    "print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
    "print(f\"Macro F1: {metrics['f1_macro']:.2f}\")\n",
    "\n",
    "for class_name in predictor.class_names:\n",
    "    if f'f1_{class_name}' in metrics:\n",
    "        print(f\"{class_name} F1: {metrics[f'f1_{class_name}']:.2f}\")\n",
    "\n",
    "if metrics['classification_report'] is not None:\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(pd.DataFrame(metrics['classification_report']).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.plot_confusion_matrix(\"Confusion_matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict optimal formats for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions = predictor.predict_formats_for_folder(\"./test\")\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "benchmark_testing = smb(\"./test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = benchmark_testing.results_df\n",
    "df_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer_testing = sma(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_formats_testing = analyzer_testing.get_optimal_formats()\n",
    "optimal_formats_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_only_testing = df_predictions[['filename', 'predicted_format']]\n",
    "predictions_only_testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_only_testing = optimal_formats_testing[['filename', 'format']]\n",
    "optimal_only_testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def compare_format_predictions(actual_df, predicted_df):\n",
    "    try:\n",
    "        for df, name, expected_column in [(actual_df, 'actual', 'format'), \n",
    "                                          (predicted_df, 'predicted', 'predicted_format')]:\n",
    "            if 'filename' not in df.columns or expected_column not in df.columns:\n",
    "                raise ValueError(f\"The {name} dataframe must have 'filename' and '{expected_column}' columns\")\n",
    "        \n",
    "        actual_formats = dict(zip(actual_df['filename'], actual_df['format']))\n",
    "        \n",
    "        correct = 0\n",
    "        total = 0\n",
    "        comparison_results = []\n",
    "        \n",
    "        for _, row in predicted_df.iterrows():\n",
    "            filename = row['filename']\n",
    "            predicted_format = row['predicted_format']\n",
    "            \n",
    "            if filename in actual_formats:\n",
    "                total += 1\n",
    "                actual_format = actual_formats[filename]\n",
    "                is_correct = (predicted_format == actual_format)\n",
    "                \n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                \n",
    "                comparison_results.append({\n",
    "                    'filename': filename,\n",
    "                    'actual_format': actual_format,\n",
    "                    'predicted_format': predicted_format,\n",
    "                    'correct_prediction': is_correct\n",
    "                })\n",
    "        \n",
    "        accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "        \n",
    "        print(f\"Total files compared: {total}\")\n",
    "        print(f\"Correct predictions: {correct}\")\n",
    "        print(f\"Incorrect predictions: {total - correct}\")\n",
    "        print(f\"Prediction accuracy: {accuracy:.2f}%\")\n",
    "        \n",
    "        return accuracy\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = compare_format_predictions(optimal_only_testing, predictions_only_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
